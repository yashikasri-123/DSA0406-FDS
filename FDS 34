import numpy as np

# Simulated true labels and predicted labels
# 1 = positive class, 0 = negative class
y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])
y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0, 1, 0])

# Confusion matrix components
TP = np.sum((y_true == 1) & (y_pred == 1))
TN = np.sum((y_true == 0) & (y_pred == 0))
FP = np.sum((y_true == 0) & (y_pred == 1))
FN = np.sum((y_true == 1) & (y_pred == 0))

# Metrics
accuracy = (TP + TN) / len(y_true)
precision = TP / (TP + FP) if (TP + FP) != 0 else 0
recall = TP / (TP + FN) if (TP + FN) != 0 else 0
f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0

# Output
print("ğŸ“Š Model Evaluation Metrics:")
print(f"âœ… Accuracy:  {accuracy:.2f}")
print(f"ğŸ¯ Precision: {precision:.2f}")
print(f"ğŸ” Recall:    {recall:.2f}")
print(f"ğŸ… F1-score:  {f1_score:.2f}")
